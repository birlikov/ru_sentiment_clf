{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-tuning Multilingual Universal Sentence Encoder",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1VAxpnNyP32kpwLzGB9MtCci3PX3Uzv3g",
      "authorship_tag": "ABX9TyN5Potl9qXnueT26SLNtqvW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/birlikov/ru_sentiment_clf/blob/master/Fine_tuning_Multilingual_Universal_Sentence_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yOSdps1tw-q",
        "colab_type": "text"
      },
      "source": [
        "### Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQztuM1rb-fG",
        "colab_type": "text"
      },
      "source": [
        "In this tutorial we will fine-tune [Multilingual Universal Sentence Encoder](https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html) on Russian twitter dataset which I found [here](http://study.mokoron.com). \n",
        "\n",
        "After that we will save the model and later deploy it as a web application using Flask. See the project in [this github repo](https://github.com/birlikov/ru_sentiment_clf).\n",
        "\n",
        "This tutorial can also be viewed in [this colab notebook](https://colab.research.google.com/drive/1VAxpnNyP32kpwLzGB9MtCci3PX3Uzv3g?usp=sharing).\n",
        "\n",
        "If doing on colab make sure to mount the drive and turn on the gpu for faster training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ-vBfZfebZo",
        "colab_type": "text"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyAQw-dtfUc5",
        "colab_type": "text"
      },
      "source": [
        "First, we download two csv files \"positive.csv\" and \"negative.csv\" from [this website](http://study.mokoron.com) and put them in folder `data` inside our project folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-guaRJTYBN1M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "30230207-aebe-40a3-cc12-2360087e58a4"
      },
      "source": [
        "import os\n",
        "my_project_folder_path = '/content/drive/My Drive/Colab Notebooks/pet_project_russian_sentiment_analysis'\n",
        "os.chdir(my_project_folder_path)\n",
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY86ChBZgJl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "ffaecb6e-d036-49d7-8649-dd2e6557edbf"
      },
      "source": [
        "os.listdir('data/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['positive.csv', 'negative.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QREQK2_NByen",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "c9457267-d830-4b43-ad53-0366dbbf0721"
      },
      "source": [
        "import pandas as pd\n",
        "positive = pd.read_csv('data/positive.csv', sep=';', header=None)\n",
        "negative = pd.read_csv('data/negative.csv', sep=';', header=None)\n",
        "positive.shape, negative.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((114911, 12), (111923, 12))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RRwyvGdh5Cr",
        "colab_type": "text"
      },
      "source": [
        "There are 114_911 positive and 111_923 negative tweets. By the way, these labeling was done in automatic manner as mentioned in the above website. Basicly they used emotocons for automatic labeling. This will be reflected in our final model as we will see later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loVVYupYB8sQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "34602669-48fe-459b-ba2e-fa658e89ff23"
      },
      "source": [
        "positive.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>408906692374446080</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>pleease_shut_up</td>\n",
              "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7569</td>\n",
              "      <td>62</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>408906692693221377</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>alinakirpicheva</td>\n",
              "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11825</td>\n",
              "      <td>59</td>\n",
              "      <td>31</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0           1                2   ...  9   10  11\n",
              "0  408906692374446080  1386325927  pleease_shut_up  ...  62  61   0\n",
              "1  408906692693221377  1386325927  alinakirpicheva  ...  59  31   2\n",
              "\n",
              "[2 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOlfFRiPCe7x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "015d7496-4055-4e06-b637-4d0ed723fdd5"
      },
      "source": [
        "negative.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>408906762813579264</td>\n",
              "      <td>1386325944</td>\n",
              "      <td>dugarchikbellko</td>\n",
              "      <td>на работе был полный пиддес :| и так каждое за...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8064</td>\n",
              "      <td>111</td>\n",
              "      <td>94</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>408906818262687744</td>\n",
              "      <td>1386325957</td>\n",
              "      <td>nugemycejela</td>\n",
              "      <td>Коллеги сидят рубятся в Urban terror, а я из-з...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>42</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0           1                2   ...   9   10  11\n",
              "0  408906762813579264  1386325944  dugarchikbellko  ...  111  94   2\n",
              "1  408906818262687744  1386325957     nugemycejela  ...   42  39   0\n",
              "\n",
              "[2 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aaa67RE9gzWB",
        "colab_type": "text"
      },
      "source": [
        "We can see that column 3 contains actual texts, so we will use them only. Although column 4 has sentiment labels (1 for positive and -1 for negetave) we will constract our own target labels as this dataframes are separete. Other columns are publication time, author name and etc. which we do not need.\n",
        "\n",
        "Also, we could do some cleaning of the text, for example removing the @nicknames, but let's leave it as it is for this tutorial)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3gSxyjtDvhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "all_sentences = np.concatenate([positive.loc[:,3].values, negative.loc[:,3].values])\n",
        "all_labels = np.array([1]*len(positive) + [0]*len(negative))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tghaUIO7jJEe",
        "colab_type": "text"
      },
      "source": [
        "Now let's split them into train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlrOAioOEYKT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "29e3e0a1-72a9-41e7-a9e5-9a506d8bc7c8"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_sentences, all_labels, test_size=0.1, random_state=42, shuffle = True, stratify = all_labels)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((204150,), (22684,), (204150,), (22684,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp20rr3ejSwS",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asvHDeuFjWNu",
        "colab_type": "text"
      },
      "source": [
        "We will use [Multilingual Universal Sentence Encoder](https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html) as our pre-trained model and add a linear classification layer with one output scalar ranging from 0 to 1, where 1 is for positive sentiment and 0 is for negative sentiment. \n",
        "\n",
        "First, let's install and import necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ-wCSltIdA8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "5af1db40-a13e-4e73-c509-e3b2ae6c3c09"
      },
      "source": [
        "!pip install tensorflow_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: tensorflow<2.4,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_text) (2.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (0.35.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.12.1)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.18.5)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.4.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.31.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (2.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (0.3.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (0.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (49.6.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (2020.6.20)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2T0TkaTJiTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_text import SentencepieceTokenizer\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Input, Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXk9GiHtkQ7X",
        "colab_type": "text"
      },
      "source": [
        "Now let's build the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPnw5CtVkWrh",
        "colab_type": "text"
      },
      "source": [
        "First, we will define an input layer. Notice how we did not deal with tokenization of the input as it is already internaly implemented in Universal Senrtence Encoder. We will just pass variable size texts as input. Pretty cool, ya?!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDYmr9S8kcii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = Input(shape = [], dtype=tf.string, name = 'input_text')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBevlqXUlV5S",
        "colab_type": "text"
      },
      "source": [
        "Now we define next layer as a keras layer and load Multilingual Universal Sentence Encoder from [tensorflow hub](https://tfhub.dev). \n",
        "\n",
        "We have two options for these models: `basic` model has 68,927,232 params and is a CNN architecture, the `large` one has 85,213,184 params and is on Transformer architecture. In this tutoril we will use the `basic` one.\n",
        "\n",
        "Notice the flag `trainable` set to `True`, this is to do fine-tuning of all model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7kioAYwJxju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3' \n",
        "##CNN 68,927,232 params\n",
        "\n",
        "# module_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3'  \n",
        "##Transformer 85,213,184 params\n",
        "\n",
        "#define keras layer\n",
        "use_layer = hub.KerasLayer(module_url, input_shape=[], dtype=tf.string, trainable=True, name='use_layer')\n",
        "\n",
        "#pass input to get features(512-dimensional vector)\n",
        "use_features = use_layer(input)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTGsFCZCnkid",
        "colab_type": "text"
      },
      "source": [
        "Next, output layer for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKO0dAUAnjiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = Dense(1, name = 'output_layer', activation='sigmoid')(use_features)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWi3gcKJnxJS",
        "colab_type": "text"
      },
      "source": [
        "Finally, build the model and see the summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwQZhRRCn03I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "7987d71d-e687-43e2-aa15-01b2aec93ed2"
      },
      "source": [
        "model = Model([input],output, name=\"sentiment_clf_model\")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sentiment_clf_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_text (InputLayer)      [(None,)]                 0         \n",
            "_________________________________________________________________\n",
            "use_layer (KerasLayer)       (None, 512)               68927232  \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 68,927,745\n",
            "Trainable params: 68,927,745\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENGxMYHjoKTS",
        "colab_type": "text"
      },
      "source": [
        "Complie the model: define loss, metrics and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiNPaCoxKNwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(1e-4))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHA8fJ7noU4a",
        "colab_type": "text"
      },
      "source": [
        "Now, let's fine-tune the model! \n",
        "\n",
        "We will train for only 5 epochs, as we will see, it is pretty enough and achieves good accuracy on validation and test sets. Another reason not to fine-tune a pre-trained model for a lot of epochs, is to prevent [catastrofic forgetting](https://en.wikipedia.org/wiki/Catastrophic_interference). Also, one can use EarlyStopping callback to prevent from this and overfitting.\n",
        "\n",
        "Anyway, let's get to the job."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_ZUK9CIMKzn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "97992f09-e4a5-4fae-cc07-5c4a8a74d499"
      },
      "source": [
        "model.fit(x = X_train, \n",
        "          y = y_train,\n",
        "          epochs = 5,\n",
        "          verbose = 1,\n",
        "          batch_size = 1024,\n",
        "          validation_split = 0.1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "180/180 [==============================] - 100s 554ms/step - loss: 0.2750 - acc: 0.9581 - val_loss: 0.1662 - val_acc: 0.9988\n",
            "Epoch 2/5\n",
            "180/180 [==============================] - 99s 550ms/step - loss: 0.1419 - acc: 0.9991 - val_loss: 0.1208 - val_acc: 0.9995\n",
            "Epoch 3/5\n",
            "180/180 [==============================] - 99s 551ms/step - loss: 0.1051 - acc: 0.9996 - val_loss: 0.0919 - val_acc: 0.9993\n",
            "Epoch 4/5\n",
            "180/180 [==============================] - 99s 551ms/step - loss: 0.0805 - acc: 0.9998 - val_loss: 0.0714 - val_acc: 0.9996\n",
            "Epoch 5/5\n",
            "180/180 [==============================] - 100s 554ms/step - loss: 0.0633 - acc: 0.9998 - val_loss: 0.0572 - val_acc: 0.9995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f213e499f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR0J6O3mrYUz",
        "colab_type": "text"
      },
      "source": [
        "As we can see, we already achieved 99% accuracy after the first epoch! That's the power of transfer learning, baby!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkql5lL2rq3O",
        "colab_type": "text"
      },
      "source": [
        "Let's evaluate on test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vOuuPJuMaex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "acccba69-d564-44dd-ea72-40b1512a7ca8"
      },
      "source": [
        "test_scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test accuracy:', test_scores[1])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.9991623759269714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uETz2lZ0rvhQ",
        "colab_type": "text"
      },
      "source": [
        "Cool. Now, let's save the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfA-LzMlPdH1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "eb95da0c-501a-4535-c3c6-7447ec60abc9"
      },
      "source": [
        "folder = 'ru_sentiment_clf/'\n",
        "model.save(folder)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ru_sentiment_clf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ru_sentiment_clf/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la3PadqJsC74",
        "colab_type": "text"
      },
      "source": [
        "Ignore the warnings)\n",
        "\n",
        "Let's see what was saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aev__zgZr-PM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "cbe9383a-1db8-46b0-977e-fc9ff5eb526e"
      },
      "source": [
        "os.listdir()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data', 'ru_sentiment_clf']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka5K7wG1sACM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "02836c8a-101e-4ddc-895c-13593fbcbea1"
      },
      "source": [
        "os.listdir('ru_sentiment_clf')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['variables', 'assets', 'saved_model.pb']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj8aTjogsPQV",
        "colab_type": "text"
      },
      "source": [
        "So we have a folder with all variables, weights, model archietecture and etc.\n",
        "\n",
        "We can completely restore the model from this folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkI4lUusQCTE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "5ef45e4e-0991-46f9-856e-f0dcae599c46"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow_text import SentencepieceTokenizer\n",
        "\n",
        "folder = 'ru_sentiment_clf/'\n",
        "reconstrucred_model = keras.models.load_model(folder)\n",
        "\n",
        "test_scores = reconstrucred_model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test accuracy again:', test_scores[1])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy again: 0.9991623759269714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKhfz1yKs0oD",
        "colab_type": "text"
      },
      "source": [
        "Cool. Reconstructed model achieved the same accuracy.\n",
        "\n",
        "Now, let's examaine couple of examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUGFb8FAsz6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "50ecaedb-e9ec-4810-c42f-cb897c1c6da7"
      },
      "source": [
        "i = 0\n",
        "one_sentence = X_test[i]\n",
        "true_label = y_test[i]\n",
        "pred = reconstrucred_model.predict([one_sentence])\n",
        "pred_label = pred[0][0]\n",
        "print(f'text: {one_sentence} \\ntrue_label:{true_label} \\npred_label:{pred_label}')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text: То ли вывихнула палец, то ли что( больно(( \n",
            "true_label:0 \n",
            "pred_label:0.055022940039634705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAQKH0xvQefe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "6741a54b-606a-4ab7-b102-670af6c1b338"
      },
      "source": [
        "i = 2\n",
        "one_sentence = X_test[i]\n",
        "true_label = y_test[i]\n",
        "pred = reconstrucred_model.predict([one_sentence])\n",
        "pred_label = pred[0][0]\n",
        "print(f'text: {one_sentence} \\ntrue_label:{true_label} \\npred_label:{pred_label}')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text: @Leon4Ik_ как меня в прошлой поездке к тебе выгнали вспомнила )) \n",
            "true_label:1 \n",
            "pred_label:0.9461257457733154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tsrsb7RtZXO",
        "colab_type": "text"
      },
      "source": [
        "Do you wanna see the effect of emoticons)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9frsPyClRF0P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "62469df2-3511-483d-f676-bd340c2259ce"
      },
      "source": [
        "i = 2\n",
        "one_sentence = X_test[i]\n",
        "\n",
        "#remove smiles\n",
        "one_sentence = one_sentence.replace(')','')\n",
        "\n",
        "true_label = y_test[i]\n",
        "pred = reconstrucred_model.predict([one_sentence])\n",
        "pred_label = pred[0][0]\n",
        "print(f'text: {one_sentence} \\ntrue_label:{true_label} \\npred_label:{pred_label}')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text: @Leon4Ik_ как меня в прошлой поездке к тебе выгнали вспомнила  \n",
            "true_label:1 \n",
            "pred_label:0.05593390390276909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw6VoII5t0Wb",
        "colab_type": "text"
      },
      "source": [
        "### Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfFh9ZIrt2yN",
        "colab_type": "text"
      },
      "source": [
        "In this tutorial we learned how we can load acutally any pre-trained model from [tensorflow hub](https://tfhub.dev) as a keras layer and fine-tune it on our own dataset and any downstream task. \n",
        "\n",
        "In this tutorial we used Russian laguage, but it could be any of those 16 languages which is supported by [Multilingual Universal Sentence Encoder](https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html) or even a multilingual dataset.\n",
        "\n",
        "Next, we use this saved model to serve as a web application using Flask in [this project](https://github.com/birlikov/ru_sentiment_clf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElEAa7y3ykEg",
        "colab_type": "text"
      },
      "source": [
        "### References\n",
        "1. Ю. В. Рубцова. Построение корпуса текстов для настройки тонового классификатора // Программные продукты и системы, 2015, №1(109), –С.72-78 ([link](http://www.swsys.ru/index.php?page=article&id=3962&lang=))\n",
        "2. Multilingual Universal Sentence Encoder for Semantic Retrieval ([link](https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html))"
      ]
    }
  ]
}